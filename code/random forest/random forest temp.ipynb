{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19bc43e3-1c6d-45fd-848a-694d32af99b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yadnesh\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation Accuracy: 13.00%\n",
      "Validation Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          Arthritis       0.18      0.07      0.10       102\n",
      "           Diabetes       0.11      0.06      0.08        99\n",
      "Digestive Disorders       0.12      0.18      0.14       114\n",
      "       Hypertension       0.12      0.17      0.14       109\n",
      "       Inflammation       0.15      0.09      0.12       127\n",
      "    Liver Disorders       0.14      0.12      0.12       130\n",
      "      Mental Stress       0.13      0.23      0.17       106\n",
      " Respiratory Issues       0.12      0.17      0.14       106\n",
      "      Skin Diseases       0.16      0.08      0.11       107\n",
      "\n",
      "           accuracy                           0.13      1000\n",
      "          macro avg       0.14      0.13      0.12      1000\n",
      "       weighted avg       0.14      0.13      0.12      1000\n",
      "\n",
      "Test Accuracy: 9.90%\n",
      "Test Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          Arthritis       0.11      0.06      0.07       144\n",
      "           Diabetes       0.13      0.05      0.07       146\n",
      "Digestive Disorders       0.12      0.13      0.12       128\n",
      "       Hypertension       0.00      0.00      0.00         0\n",
      "       Inflammation       0.13      0.06      0.08       141\n",
      "    Liver Disorders       0.00      0.00      0.00         0\n",
      "      Mental Stress       0.16      0.19      0.17       143\n",
      " Respiratory Issues       0.14      0.12      0.13       155\n",
      "      Skin Diseases       0.13      0.09      0.11       143\n",
      "\n",
      "           accuracy                           0.10      1000\n",
      "          macro avg       0.10      0.08      0.08      1000\n",
      "       weighted avg       0.13      0.10      0.11      1000\n",
      "\n",
      "       Feature  Importance\n",
      "0    Feature 0    0.096343\n",
      "80  Feature 80    0.021345\n",
      "1    Feature 1    0.019533\n",
      "78  Feature 78    0.018432\n",
      "79  Feature 79    0.018229\n",
      "..         ...         ...\n",
      "5    Feature 5    0.007763\n",
      "27  Feature 27    0.007707\n",
      "23  Feature 23    0.007436\n",
      "24  Feature 24    0.007372\n",
      "26  Feature 26    0.007131\n",
      "\n",
      "[81 rows x 2 columns]\n",
      "Execution Time: 50.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yadnesh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Yadnesh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Yadnesh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "def load_and_preprocess_data(file_path, preprocessor=None):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()  # Clean column names\n",
    "\n",
    "    # Check if 'Disease' column exists\n",
    "    if 'Disease' not in df.columns:\n",
    "        raise KeyError(\"Column 'Disease' not found in dataset.\")\n",
    "\n",
    "    # Create a new feature: SymptomSeverity\n",
    "    df['SymptomSeverity'] = df['Symptoms/Condition'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop('Disease', axis=1)\n",
    "    y = df['Disease']\n",
    "\n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = ['Dosage (mg)', 'SymptomSeverity']\n",
    "    categorical_cols = [col for col in X.columns if col not in numerical_cols]\n",
    "\n",
    "    # Create preprocessing pipelines\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    if preprocessor is None:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_cols),\n",
    "                ('cat', categorical_transformer, categorical_cols)\n",
    "            ]\n",
    "        )\n",
    "        # Fit the preprocessor on the data\n",
    "        X_preprocessed = preprocessor.fit_transform(X)\n",
    "    else:\n",
    "        # Apply the existing preprocessor\n",
    "        X_preprocessed = preprocessor.transform(X)\n",
    "\n",
    "    return X_preprocessed, y, preprocessor\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_random_forest(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'criterion': ['gini'],\n",
    "        'max_features': ['auto']\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', verbose=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X, y, dataset_type=\"Validation\"):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    report = classification_report(y, y_pred)\n",
    "\n",
    "    print(f\"{dataset_type} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{dataset_type} Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return accuracy, report\n",
    "\n",
    "# Function to display feature importance\n",
    "def display_feature_importance(model, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    print(feature_importance_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()  # Start time for profiling\n",
    "\n",
    "    # File paths of the datasets\n",
    "    train_file_path = 'synthetic_detailed_ayurveda_dataset.csv'  # Detailed dataset for training\n",
    "    test_file_path = 'synthetic_ayurveda_dataset.csv'  # Simpler dataset for validation/testing\n",
    "\n",
    "    # Step 1: Load and preprocess the training dataset\n",
    "    X_train, y_train, preprocessor = load_and_preprocess_data(train_file_path)\n",
    "\n",
    "    # Step 2: Split training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = split_data(X_train, y_train)\n",
    "\n",
    "    # Step 3: Train Random Forest model using the training dataset\n",
    "    rf_classifier = train_random_forest(X_train, y_train)\n",
    "\n",
    "    # Step 4: Load and preprocess the test dataset using the existing preprocessor\n",
    "    X_test, y_test, _ = load_and_preprocess_data(test_file_path, preprocessor)\n",
    "\n",
    "    # Step 5: Evaluate the model on the validation set\n",
    "    evaluate_model(rf_classifier, X_val, y_val, dataset_type=\"Validation\")\n",
    "\n",
    "    # Step 6: Evaluate the model on the test set\n",
    "    evaluate_model(rf_classifier, X_test, y_test, dataset_type=\"Test\")\n",
    "\n",
    "    # Step 7: Display feature importance from the trained model\n",
    "    feature_names = [f\"Feature {i}\" for i in range(X_train.shape[1])]  # Replace with actual feature names if available\n",
    "    display_feature_importance(rf_classifier, feature_names)\n",
    "\n",
    "    end_time = time.time()  # End time for profiling\n",
    "    print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d36938-7427-4d7f-a0a8-c37cb65f755d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
